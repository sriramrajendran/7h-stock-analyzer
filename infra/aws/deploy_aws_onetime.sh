#!/bin/bash
set -e

# Parse command line arguments
QUICK_MODE=false
LAMBDA_ONLY=false
FRONTEND_ONLY=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --quick)
            QUICK_MODE=true
            shift
            ;;
        --lambda-only)
            LAMBDA_ONLY=true
            shift
            ;;
        --frontend-only)
            FRONTEND_ONLY=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--quick] [--lambda-only] [--frontend-only]"
            exit 1
            ;;
    esac
done

echo "üöÄ Deploying 7H Stock Analyzer to AWS (Cost-Optimized)..."
if [ "$QUICK_MODE" = true ]; then
    echo "‚ö° Quick mode enabled - Lambda-only update"
fi

# Load environment variables
if [ ! -f ../../.env.local ]; then
    echo "‚ùå .env.local not found. Please create .env.local with AWS configuration."
    exit 1
fi

export $(grep -v '^#' ../../.env.local | xargs)

# Check if AWS CLI is configured
if ! aws sts get-caller-identity &>/dev/null; then
    echo "‚ùå AWS CLI not configured. Please run 'aws configure' first."
    exit 1
fi

# Get AWS account info
AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
AWS_REGION=${AWS_REGION:-us-east-1}

echo "üìã Deployment Configuration:"
echo "  Account: $AWS_ACCOUNT"
echo "  Region: $AWS_REGION"
echo "  Environment: ${ENVIRONMENT:-dev}"

# Set environment-specific variables
ENVIRONMENT=${ENVIRONMENT:-dev}
STACK_NAME="7h-stock-analyzer-${ENVIRONMENT}"

# Quick mode: Skip infrastructure setup and directly update Lambda
if [ "$QUICK_MODE" = true ]; then
    echo "‚ö° Quick Lambda Update Mode"
    echo "  Skipping infrastructure setup..."
    echo "  Updating Lambda function code directly..."
    
    # Get existing Lambda function name from stack
    LAMBDA_FUNCTION_NAME=$(aws lambda list-functions \
        --query "Functions[?contains(FunctionName, 'StockAnalyzerFunction')].FunctionName" \
        --output text \
        --region $AWS_REGION)
    
    if [ -z "$LAMBDA_FUNCTION_NAME" ] || [ "$LAMBDA_FUNCTION_NAME" = "None" ]; then
        echo "‚ùå Lambda function not found. Please run full deployment first."
        exit 1
    fi
    
    echo "üîß Found Lambda function: $LAMBDA_FUNCTION_NAME"
    
    # Build application package (quick version)
    echo "üì¶ Building application package..."
    rm -rf ../build/package
    mkdir -p ../build/package
    
    # Copy only necessary files
    cp -r /Users/sriramrajendran/7_projects/7h-stock-analyzer/backend/app ../build/package/
    cp /Users/sriramrajendran/7_projects/7h-stock-analyzer/backend/requirements.txt ../build/package/
    
    # Create zip
    cd ../build/package
    zip -r ../../stock-analyzer-lambda-quick.zip . -x "*.pyc" "*.pyo" "__pycache__/*"
    cd ../..
    
    PACKAGE_SIZE=$(du -h stock-analyzer-lambda-quick.zip | cut -f1)
    echo "‚úÖ Application package created: $PACKAGE_SIZE"
    
    # Update Lambda function code directly
    echo "üîÑ Updating Lambda function code..."
    aws lambda update-function-code \
        --function-name "$LAMBDA_FUNCTION_NAME" \
        --zip-file fileb://stock-analyzer-lambda-quick.zip \
        --region $AWS_REGION
    
    # Wait for update to complete
    echo "‚è≥ Waiting for Lambda update to complete..."
    aws lambda wait function-updated \
        --function-name "$LAMBDA_FUNCTION_NAME" \
        --region $AWS_REGION
    
    # Test the updated function
    echo "üß™ Testing updated Lambda function..."
    API_URL=$(aws cloudformation describe-stacks \
        --stack-name $STACK_NAME \
        --region $AWS_REGION \
        --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerApi`].OutputValue' \
        --output text)
    
    if [ -n "$API_URL" ] && [ "$API_URL" != "None" ]; then
        echo "üåê Testing API endpoint: $API_URL/health"
        HEALTH_RESPONSE=$(curl -s -w "%{http_code}" "$API_URL/health" || echo "000")
        
        if [[ "$HEALTH_RESPONSE" == *"200"* ]]; then
            echo "‚úÖ Lambda function updated and tested successfully"
        else
            echo "‚ö†Ô∏è  Lambda updated but health check failed: $HEALTH_RESPONSE"
        fi
    else
        echo "‚ö†Ô∏è  Could not get API URL for testing"
    fi
    
    # Clean up
    rm -f stock-analyzer-lambda-quick.zip
    rm -rf ../build/package
    
    echo ""
    echo "üéâ Quick Lambda deployment completed!"
    echo "‚ö° Deployment time: ~30-60 seconds (vs 15+ minutes for full deployment)"
    echo ""
    echo "üìã Quick deployment info:"
    echo "  Function: $LAMBDA_FUNCTION_NAME"
    echo "  API URL: $API_URL"
    echo "  Package size: $PACKAGE_SIZE"
    echo ""
    echo "üí° For full infrastructure changes, run without --quick flag"
    
    exit 0
fi

# Cost optimization settings
MEMORY_SIZE=${MEMORY_SIZE:-512}  # Reduced from 1024MB
TIMEOUT=${TIMEOUT:-180}  # Reduced from 300s
RESERVED_CONCURRENCY=${RESERVED_CONCURRENCY:-2}  # Reduced from 5

echo "üí∞ Cost Optimization Settings:"
echo "  Memory: ${MEMORY_SIZE}MB"
echo "  Timeout: ${TIMEOUT}s"
echo "  Concurrency: ${RESERVED_CONCURRENCY}"

# Create S3 bucket for deployment artifacts
DEPLOYMENT_BUCKET="$S3_BUCKET_NAME_PROD"
echo "ü™£ Creating deployment bucket: $DEPLOYMENT_BUCKET"

if ! aws s3 ls "s3://$DEPLOYMENT_BUCKET" &>/dev/null; then
    aws s3 mb "s3://$DEPLOYMENT_BUCKET" --region $AWS_REGION
    echo "‚úÖ Created deployment bucket"
else
    echo "‚ÑπÔ∏è  Deployment bucket already exists"
fi

# Build Lambda layer
echo "üì¶ Building Lambda layer..."

# Create build directories
echo "üèóÔ∏è Creating build directories..."
mkdir -p ../build/layer/python
mkdir -p ../build/package

# Install production dependencies
echo "Installing production dependencies..."
pip install \
    --target ../build/layer/python \
    -r ../../backend/requirements.txt

# Optimize layer size
echo "Optimizing layer size..."
find ../build/layer/python -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
find ../build/layer/python -name "*.pyc" -delete 2>/dev/null || true
find ../build/layer/python -name "*.pyo" -delete 2>/dev/null || true
find ../build/layer/python -name "*.pyd" -delete 2>/dev/null || true
find ../build/layer/python -name "*.so" -delete 2>/dev/null || true

# Remove unnecessary files
rm -rf ../build/layer/python/*/tests 2>/dev/null || true
rm -rf ../build/layer/python/*/.github 2>/dev/null || true
rm -rf ../build/layer/python/*/doc 2>/dev/null || true

# Compress layer
echo "Compressing layer..."
cd ../build/layer
zip -r ../../stock-analyzer-layer.zip python -x "*.pyc" "*.pyo" "__pycache__/*"
cd ../..

LAYER_SIZE=$(du -h stock-analyzer-layer.zip | cut -f1)
echo "‚úÖ Lambda layer created: $LAYER_SIZE"

# Build application package
echo "üì¶ Building application package..."
rm -rf ../build/package
mkdir -p ../build/package

# Copy only necessary files
cp -r /Users/sriramrajendran/7_projects/7h-stock-analyzer/backend/app ../build/package/
cp /Users/sriramrajendran/7_projects/7h-stock-analyzer/backend/requirements.txt ../build/package/

# Create zip
cd ../build/package
zip -r ../../stock-analyzer-lambda.zip . -x "*.pyc" "*.pyo" "__pycache__/*"
cd ../..

PACKAGE_SIZE=$(du -h stock-analyzer-lambda.zip | cut -f1)
echo "‚úÖ Application package created: $PACKAGE_SIZE"

# Upload to S3
echo "üì§ Uploading to S3..."
aws s3 cp stock-analyzer-layer.zip "s3://$DEPLOYMENT_BUCKET/"
aws s3 cp ../../stock-analyzer-lambda.zip "s3://$DEPLOYMENT_BUCKET/"

# Deploy with SAM (cost-optimized template)
echo "üöÄ Deploying with SAM..."
sam deploy \
    --template-file template.yaml \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --s3-bucket $DEPLOYMENT_BUCKET \
    --parameter-overrides \
        Environment=$ENVIRONMENT \
    --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM CAPABILITY_AUTO_EXPAND \
    --no-confirm-changeset \
    --no-fail-on-empty-changeset

# Get stack outputs
echo "üìã Getting stack outputs..."
API_URL=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerApi`].OutputValue' \
    --output text)

S3_BUCKET=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockDataBucket`].OutputValue' \
    --output text)

LAMBDA_ARN=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerFunction`].OutputValue' \
    --output text)

API_KEY=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerApiKey`].OutputValue' \
    --output text)

# Store API key in SSM Parameter Store (secure, not hardcoded)
echo "üîê Storing API key in SSM Parameter Store..."
if [ -z "$API_KEY" ]; then
    echo "‚ùå API_KEY not found in environment variables"
    echo "Please set API_KEY in your .env.local file"
    exit 1
fi

aws ssm put-parameter \
    --name "/stock-analyzer/api-key" \
    --value "$API_KEY" \
    --type "SecureString" \
    --description "Stock Analyzer API Key" \
    --overwrite

# Set up CloudWatch billing alerts
echo "üí∞ Setting up CloudWatch billing alerts..."
SNS_TOPIC_ARN=$(aws sns create-topic --name "stock-analyzer-billing-alerts" --query "TopicArn" --output text)

aws cloudwatch put-metric-alarm \
    --alarm-name "Stock-Analyzer-High-Cost" \
    --alarm-description "Alert when monthly costs exceed $10" \
    --metric-name "EstimatedCharges" \
    --namespace "AWS/Billing" \
    --statistic "Maximum" \
    --period 21600 \
    --evaluation-periods 1 \
    --threshold 10 \
    --comparison-operator "GreaterThanThreshold" \
    --unit "USD" \
    --alarm-actions "$SNS_TOPIC_ARN"

echo "‚úÖ CloudWatch billing alarm created with $10 threshold"

echo ""
echo "üéâ Deployment completed successfully!"
echo ""
echo "üìã Stack Information:"
echo "  Stack Name: $STACK_NAME"
echo "  API URL: $API_URL"
echo "  S3 Bucket: $S3_BUCKET"
echo "  Lambda ARN: $LAMBDA_ARN"
echo "  API Key: $API_KEY"
echo "  CloudFront: https://d224ztwcw6zi6e.cloudfront.net"
echo "  Billing Alert: $10 threshold configured"
echo "  Rate Limiting: Route-specific throttling enabled"
echo ""
echo "üß™ Test the deployment:"
echo "  curl -H \"X-API-Key: \$API_KEY\" $API_URL/health"
echo "  curl -H \"X-API-Key: \$API_KEY\" $API_URL/recommendations"
echo "  curl -H \"X-API-Key: \$API_KEY\" $API_URL/history/2024-02-07"
echo ""
echo "üåê Frontend URLs:"
echo "  HTTPS CDN: https://d224ztwcw6zi6e.cloudfront.net"
echo "  S3 Direct: http://7h-stock-analyzer.s3-website-us-east-1.amazonaws.com"
echo ""
echo "üí∞ Cost Optimization Applied:"
echo "  ‚úÖ Reduced memory to ${MEMORY_SIZE}MB"
echo "  ‚úÖ Reduced timeout to ${TIMEOUT}s"
echo "  ‚úÖ Limited concurrency to ${RESERVED_CONCURRENCY}"
echo "  ‚úÖ Disabled VPC (reduces cost)"
echo "  ‚úÖ Optimized layer size ($LAYER_SIZE)"
echo "  ‚úÖ Rate limiting configured (HTTP API - FREE)"
echo ""
echo "üìä Estimated Monthly Cost: < $15"
echo "  - Lambda: ~$8 (based on 100k invocations/month)"
echo "  - S3: ~$3 (storage + requests)"
echo "  - API Gateway: ~$4 (1M requests/month)"
echo "  - Rate Limiting: $0.00 (HTTP API throttling included)"
echo "  - CloudWatch Logs: ~$0.50 (log ingestion)"
echo "  - CloudWatch Alarms: ~$0.10 (billing alerts)"
echo "  - CloudFront: ~$0.00 (free tier: 1TB + 10M requests)"
echo "  - Total: ~$15.60/month (conservative high-usage estimate)"
echo "  - Light usage: ~$2.46/month (61 invocations/month)"

# Clean up local files
echo "üßπ Cleaning up local files..."
rm -f stock-analyzer-layer.zip stock-analyzer-lambda.zip
rm -rf ../build

# Clean up old Lambda layer versions (storage optimization)
echo "üßπ Cleaning up old Lambda layer versions..."
LAYER_NAME="StockAnalyzerDependencies"
LATEST_VERSION=$(aws lambda list-layer-versions --layer-name $LAYER_NAME --query 'max(LayerVersions[].Version)' --output text 2>/dev/null || echo "0")

if [ "$LATEST_VERSION" != "0" ] && [ "$LATEST_VERSION" -gt 1 ]; then
    echo "Found $LATEST_VERSION layer versions, cleaning up old versions..."
    
    # Delete all versions except the latest one
    for ((i=1; i<LATEST_VERSION; i++)); do
        echo "Deleting layer version $i..."
        aws lambda delete-layer-version --layer-name $LAYER_NAME --version-number $i 2>/dev/null && echo "‚úÖ Deleted version $i" || echo "‚ùå Failed to delete version $i"
    done
    
    echo "‚úÖ Layer cleanup completed. Kept only version $LATEST_VERSION"
    echo "üí∞ Storage cost reduced by ~90%"
else
    echo "‚úÖ No old layer versions to clean up (latest version: $LATEST_VERSION)"
fi

echo "‚úÖ Deployment cleanup completed"

# Setup weekly reconciliation monitoring
echo ""
echo "üîÑ Setting up weekly reconciliation monitoring..."

# Create S3 prefix for reconciliation data
echo "üìÅ Creating reconciliation data structure..."
aws s3api put-object \
    --bucket "$S3_BUCKET_NAME_PROD" \
    --key "recon/.gitkeep" \
    --content-type "application/octet-stream" \
    --region $AWS_REGION || echo "‚ÑπÔ∏è  Recon directory already exists"

# Test reconciliation endpoint
echo "üß™ Testing reconciliation endpoint..."
API_URL=$(aws cloudformation describe-stacks \
    --stack-name "$STACK_NAME" \
    --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \
    --output text)

if [ -n "$API_URL" ]; then
    echo "üåê API URL: $API_URL"
    
    # Test recon endpoint (will show 0 initially, but confirms it works)
    echo "üìä Testing reconciliation API..."
    RECON_RESPONSE=$(curl -s -w "%{http_code}" "$API_URL/recon/summary" || echo "000")
    
    if [[ "$RECON_RESPONSE" == *"200"* ]]; then
        echo "‚úÖ Reconciliation API endpoint is working"
    else
        echo "‚ö†Ô∏è  Reconciliation API test failed (expected for new deployment)"
    fi
    
    echo ""
    echo "üìã Weekly Reconciliation Details:"
    echo "  üïê Schedule: Every Sunday at 6:00 PM EST (23:00 UTC)"
    echo "  üìä Data: Tracks profit targets vs stop losses"
    echo "  üìà Metrics: Days to target, success rates, performance by type"
    echo "  üîó Endpoint: $API_URL/recon/summary"
    echo "  üóÇÔ∏è  Storage: s3://$S3_BUCKET_NAME_PROD/recon/daily/"
    echo ""
    echo "üéØ To view reconciliation data:"
    echo "  curl -s $API_URL/recon/summary | jq ."
    echo "  curl -s $API_URL/recon/daily/\$(date +%Y-%m-%d) | jq ."
else
    echo "‚ö†Ô∏è  Could not get API URL - reconciliation may need manual testing"
fi

echo ""
echo "‚úÖ Weekly reconciliation setup completed!"

# Verify rate limiting configuration
echo ""
echo "üîí Verifying rate limiting configuration..."

# Get API Gateway ID for rate limiting verification
API_GATEWAY_ID=$(aws apigatewayv2 get-apis \
    --query "Items[?Name=='7h-stock-analyzer'].ApiId" \
    --output text \
    --region $AWS_REGION 2>/dev/null || echo "")

if [ -n "$API_GATEWAY_ID" ] && [ "$API_GATEWAY_ID" != "None" ]; then
    echo "üö¶ API Gateway ID: $API_GATEWAY_ID"
    
    # Get current stage configuration
    echo "üìã Current rate limiting configuration:"
    aws apigatewayv2 get-stage \
        --api-id "$API_GATEWAY_ID" \
        --stage-name "\$default" \
        --region $AWS_REGION \
        --query 'RouteSettings' \
        --output table 2>/dev/null || echo "  ‚ÑπÔ∏è  Rate limiting configured in template"
    
    echo ""
    echo "üéØ Rate limiting limits by endpoint:"
    echo "  GET /recommendations: 20 req/s burst 10"
    echo "  POST /run-now: 10 req/s burst 5"
    echo "  GET /history/{date}: 30 req/s burst 15"
    echo "  GET /history/{date}/enhanced: 10 req/s burst 5"
    echo "  POST /recon/run: 5 req/s burst 2"
    echo "  GET /config: 15 req/s burst 10"
    echo "  POST /config/update: 10 req/s burst 5"
    echo "  Default: 100 req/s burst 20"
    
    echo ""
    echo "üß™ To test rate limiting:"
    echo "  # Test normal operation (should work)"
    echo "  curl -H \"X-API-Key: \$API_KEY\" $API_URL/health"
    echo ""
    echo "  # Test throttling (rapid requests may get 429)"
    echo "  for i in {1..10}; do curl -H \"X-API-Key: \$API_KEY\" $API_URL/health; done"
else
    echo "‚ö†Ô∏è  Could not verify rate limiting - may need manual check"
fi

echo ""
echo "üîí Rate limiting verification completed!"
