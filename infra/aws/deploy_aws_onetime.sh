#!/bin/bash
set -e

echo "ðŸš€ Deploying 7H Stock Analyzer to AWS (Cost-Optimized)..."

# Check if AWS CLI is configured
if ! aws sts get-caller-identity &>/dev/null; then
    echo "âŒ AWS CLI not configured. Please run 'aws configure' first."
    exit 1
fi

# Get AWS account info
AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
AWS_REGION=${AWS_REGION:-us-east-1}

echo "ðŸ“‹ Deployment Configuration:"
echo "  Account: $AWS_ACCOUNT"
echo "  Region: $AWS_REGION"
echo "  Environment: ${ENVIRONMENT:-dev}"

# Set environment-specific variables
ENVIRONMENT=${ENVIRONMENT:-dev}
STACK_NAME="7h-stock-analyzer-${ENVIRONMENT}"

# Cost optimization settings
MEMORY_SIZE=${MEMORY_SIZE:-512}  # Reduced from 1024MB
TIMEOUT=${TIMEOUT:-180}  # Reduced from 300s
RESERVED_CONCURRENCY=${RESERVED_CONCURRENCY:-2}  # Reduced from 5

echo "ðŸ’° Cost Optimization Settings:"
echo "  Memory: ${MEMORY_SIZE}MB"
echo "  Timeout: ${TIMEOUT}s"
echo "  Concurrency: ${RESERVED_CONCURRENCY}"

# Create S3 bucket for deployment artifacts
DEPLOYMENT_BUCKET="7h-stock-analyzer-deploy"
echo "ðŸª£ Creating deployment bucket: $DEPLOYMENT_BUCKET"

if ! aws s3 ls "s3://$DEPLOYMENT_BUCKET" &>/dev/null; then
    aws s3 mb "s3://$DEPLOYMENT_BUCKET" --region $AWS_REGION
    echo "âœ… Created deployment bucket"
else
    echo "â„¹ï¸  Deployment bucket already exists"
fi

# Build Lambda layer
echo "ðŸ“¦ Building Lambda layer..."
rm -rf ../layer/python
mkdir -p ../layer/python

# Install only essential dependencies for production
echo "Installing production dependencies..."
pip install --platform manylinux2014_x86_64 --only-binary=:all: \
    --target layer/python \
    -r backend/requirements.txt

# Optimize layer size
echo "Optimizing layer size..."
find ../layer/python -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
find ../layer/python -name "*.pyc" -delete 2>/dev/null || true
find ../layer/python -name "*.pyo" -delete 2>/dev/null || true
find ../layer/python -name "*.pyd" -delete 2>/dev/null || true
find ../layer/python -name "*.so" -delete 2>/dev/null || true

# Remove unnecessary files
rm -rf ../layer/python/*/tests 2>/dev/null || true
rm -rf ../layer/python/*/.github 2>/dev/null || true
rm -rf ../layer/python/*/doc 2>/dev/null || true

# Compress layer
echo "Compressing layer..."
cd ../layer
zip -r ../stock-analyzer-layer.zip python -x "*.pyc" "*.pyo" "__pycache__/*"
cd ..

LAYER_SIZE=$(du -h stock-analyzer-layer.zip | cut -f1)
echo "âœ… Lambda layer created: $LAYER_SIZE"

# Build application package
echo "ðŸ“¦ Building application package..."
rm -rf package
mkdir -p package

# Copy only necessary files
cp -r backend/app package/
cp backend/requirements.txt package/

# Create zip
cd package
zip -r ../stock-analyzer-lambda.zip . -x "*.pyc" "*.pyo" "__pycache__/*"
cd ..

PACKAGE_SIZE=$(du -h stock-analyzer-lambda.zip | cut -f1)
echo "âœ… Application package created: $PACKAGE_SIZE"

# Upload to S3
echo "ðŸ“¤ Uploading to S3..."
aws s3 cp stock-analyzer-layer.zip "s3://$DEPLOYMENT_BUCKET/"
aws s3 cp stock-analyzer-lambda.zip "s3://$DEPLOYMENT_BUCKET/"

# Deploy with SAM (cost-optimized template)
echo "ðŸš€ Deploying with SAM..."
sam deploy \
    --template-file template.yaml \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --s3-bucket $DEPLOYMENT_BUCKET \
    --parameter-overrides \
        Environment=$ENVIRONMENT \
        MemorySize=$MEMORY_SIZE \
        Timeout=$TIMEOUT \
        ReservedConcurrency=$RESERVED_CONCURRENCY \
        EnableVpc=false \
    --capabilities CAPABILITY_IAM CAPABILITY_AUTO_EXPAND \
    --no-confirm-changeset \
    --no-fail-on-empty-changeset

# Get stack outputs
echo "ðŸ“‹ Getting stack outputs..."
API_URL=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerApi`].OutputValue' \
    --output text)

S3_BUCKET=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockDataBucket`].OutputValue' \
    --output text)

LAMBDA_ARN=$(aws cloudformation describe-stacks \
    --stack-name $STACK_NAME \
    --region $AWS_REGION \
    --query 'Stacks[0].Outputs[?OutputKey==`StockAnalyzerFunction`].OutputValue' \
    --output text)

echo ""
echo "ðŸŽ‰ Deployment completed successfully!"
echo ""
echo "ðŸ“‹ Stack Information:"
echo "  Stack Name: $STACK_NAME"
echo "  API URL: $API_URL"
echo "  S3 Bucket: $S3_BUCKET"
echo "  Lambda ARN: $LAMBDA_ARN"
echo ""
echo "ðŸ§ª Test the deployment:"
echo "  curl $API_URL/health"
echo "  curl $API_URL/analysis/AAPL"
echo ""
echo "ðŸ’° Cost Optimization Applied:"
echo "  âœ… Reduced memory to ${MEMORY_SIZE}MB"
echo "  âœ… Reduced timeout to ${TIMEOUT}s"
echo "  âœ… Limited concurrency to ${RESERVED_CONCURRENCY}"
echo "  âœ… Disabled VPC (reduces cost)"
echo "  âœ… Optimized layer size ($LAYER_SIZE)"
echo ""
echo "ðŸ“Š Estimated Monthly Cost: < $15"
echo "  - Lambda: ~$8 (based on 100k invocations/month)"
echo "  - S3: ~$3 (storage + requests)"
echo "  - API Gateway: ~$4 (1M requests/month)"

# Clean up local files
echo "ðŸ§¹ Cleaning up local files..."
rm -f stock-analyzer-layer.zip stock-analyzer-lambda.zip
rm -rf layer package

echo "âœ… Deployment cleanup completed"

# Setup weekly reconciliation monitoring
echo ""
echo "ðŸ”„ Setting up weekly reconciliation monitoring..."

# Create S3 prefix for reconciliation data
echo "ðŸ“ Creating reconciliation data structure..."
aws s3api put-object \
    --bucket "7h-stock-analyzer-${ENVIRONMENT}" \
    --key "recon/.gitkeep" \
    --content-type "application/octet-stream" \
    --region $AWS_REGION || echo "â„¹ï¸  Recon directory already exists"

# Test reconciliation endpoint
echo "ðŸ§ª Testing reconciliation endpoint..."
API_URL=$(aws cloudformation describe-stacks \
    --stack-name "$STACK_NAME" \
    --query 'Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue' \
    --output text)

if [ -n "$API_URL" ]; then
    echo "ðŸŒ API URL: $API_URL"
    
    # Test recon endpoint (will show 0 initially, but confirms it works)
    echo "ðŸ“Š Testing reconciliation API..."
    RECON_RESPONSE=$(curl -s -w "%{http_code}" "$API_URL/recon/summary" || echo "000")
    
    if [[ "$RECON_RESPONSE" == *"200"* ]]; then
        echo "âœ… Reconciliation API endpoint is working"
    else
        echo "âš ï¸  Reconciliation API test failed (expected for new deployment)"
    fi
    
    echo ""
    echo "ðŸ“‹ Weekly Reconciliation Details:"
    echo "  ðŸ• Schedule: Every Sunday at 6:00 PM EST (23:00 UTC)"
    echo "  ðŸ“Š Data: Tracks profit targets vs stop losses"
    echo "  ðŸ“ˆ Metrics: Days to target, success rates, performance by type"
    echo "  ðŸ”— Endpoint: $API_URL/recon/summary"
    echo "  ðŸ—‚ï¸  Storage: s3://7h-stock-analyzer-${ENVIRONMENT}/recon/daily/"
    echo ""
    echo "ðŸŽ¯ To view reconciliation data:"
    echo "  curl -s $API_URL/recon/summary | jq ."
    echo "  curl -s $API_URL/recon/daily/\$(date +%Y-%m-%d) | jq ."
else
    echo "âš ï¸  Could not get API URL - reconciliation may need manual testing"
fi

echo ""
echo "âœ… Weekly reconciliation setup completed!"
